{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import cuda\n",
    "import torch\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "embed_model_id = 'sentence-transformers/all-MiniLM-L6-v2'\n",
    "\n",
    "device = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'\n",
    "\n",
    "embed_model = HuggingFaceEmbeddings(\n",
    "    model_name=embed_model_id,\n",
    "    model_kwargs={'device': device},\n",
    "    encode_kwargs={'device': device, 'batch_size': 32}\n",
    ")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "# find .env automagically by walking up directories until it's found\n",
    "dotenv_path = find_dotenv()\n",
    "\n",
    "# load up the entries as environment variables\n",
    "load_dotenv(dotenv_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone import Pinecone\n",
    "from pinecone import ServerlessSpec\n",
    "\n",
    "pinecone_api_key = os.environ.get(\"PINECONE_API_KEY\")\n",
    "# configure client\n",
    "pc = Pinecone(api_key=pinecone_api_key)\n",
    "\n",
    "spec = ServerlessSpec(\n",
    "        cloud=\"aws\",\n",
    "        region=\"us-east-1\"\n",
    "    ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 2 doc embeddings, each with a dimensionality of 384.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "docs = [\n",
    "    \"this is one document\",\n",
    "    \"and another document\"\n",
    "]\n",
    "\n",
    "embeddings = embed_model.embed_documents(docs)\n",
    "\n",
    "print(f\"We have {len(embeddings)} doc embeddings, each with \"\n",
    "      f\"a dimensionality of {len(embeddings[0])}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "index_name = 'llama-2-rag-proto'\n",
    "\n",
    "import time\n",
    "\n",
    "# check if index already exists (it shouldn't if this is first time)\n",
    "if index_name not in pc.list_indexes().names():\n",
    "    # if does not exist, create index\n",
    "    pc.create_index(\n",
    "        index_name,\n",
    "        dimension=len(embeddings[0]),\n",
    "        metric='cosine',\n",
    "        spec=spec\n",
    "    )\n",
    "    # wait for index to be initialized\n",
    "    while not pc.describe_index(index_name).status['ready']:\n",
    "        time.sleep(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dimension': 384,\n",
       " 'index_fullness': 0.0,\n",
       " 'namespaces': {'': {'vector_count': 4838}},\n",
       " 'total_vector_count': 4838}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# connect to index\n",
    "index = pc.Index(index_name)\n",
    "# view index stats\n",
    "index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "data = load_dataset(\n",
    "    'jamescalam/llama-2-arxiv-papers-chunked',\n",
    "    split='train'\n",
    ")\n",
    "data = data.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doi</th>\n",
       "      <th>chunk-id</th>\n",
       "      <th>chunk</th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>summary</th>\n",
       "      <th>source</th>\n",
       "      <th>authors</th>\n",
       "      <th>categories</th>\n",
       "      <th>comment</th>\n",
       "      <th>journal_ref</th>\n",
       "      <th>primary_category</th>\n",
       "      <th>published</th>\n",
       "      <th>updated</th>\n",
       "      <th>references</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1102.0183</td>\n",
       "      <td>0</td>\n",
       "      <td>High-Performance Neural Networks\\nfor Visual O...</td>\n",
       "      <td>1102.0183</td>\n",
       "      <td>High-Performance Neural Networks for Visual Ob...</td>\n",
       "      <td>We present a fast, fully parameterizable GPU i...</td>\n",
       "      <td>http://arxiv.org/pdf/1102.0183</td>\n",
       "      <td>[Dan C. Cireşan, Ueli Meier, Jonathan Masci, L...</td>\n",
       "      <td>[cs.AI, cs.NE]</td>\n",
       "      <td>12 pages, 2 figures, 5 tables</td>\n",
       "      <td>None</td>\n",
       "      <td>cs.AI</td>\n",
       "      <td>20110201</td>\n",
       "      <td>20110201</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1102.0183</td>\n",
       "      <td>1</td>\n",
       "      <td>January 2011\\nAbstract\\nWe present a fast, ful...</td>\n",
       "      <td>1102.0183</td>\n",
       "      <td>High-Performance Neural Networks for Visual Ob...</td>\n",
       "      <td>We present a fast, fully parameterizable GPU i...</td>\n",
       "      <td>http://arxiv.org/pdf/1102.0183</td>\n",
       "      <td>[Dan C. Cireşan, Ueli Meier, Jonathan Masci, L...</td>\n",
       "      <td>[cs.AI, cs.NE]</td>\n",
       "      <td>12 pages, 2 figures, 5 tables</td>\n",
       "      <td>None</td>\n",
       "      <td>cs.AI</td>\n",
       "      <td>20110201</td>\n",
       "      <td>20110201</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         doi chunk-id                                              chunk  \\\n",
       "0  1102.0183        0  High-Performance Neural Networks\\nfor Visual O...   \n",
       "1  1102.0183        1  January 2011\\nAbstract\\nWe present a fast, ful...   \n",
       "\n",
       "          id                                              title  \\\n",
       "0  1102.0183  High-Performance Neural Networks for Visual Ob...   \n",
       "1  1102.0183  High-Performance Neural Networks for Visual Ob...   \n",
       "\n",
       "                                             summary  \\\n",
       "0  We present a fast, fully parameterizable GPU i...   \n",
       "1  We present a fast, fully parameterizable GPU i...   \n",
       "\n",
       "                           source  \\\n",
       "0  http://arxiv.org/pdf/1102.0183   \n",
       "1  http://arxiv.org/pdf/1102.0183   \n",
       "\n",
       "                                             authors      categories  \\\n",
       "0  [Dan C. Cireşan, Ueli Meier, Jonathan Masci, L...  [cs.AI, cs.NE]   \n",
       "1  [Dan C. Cireşan, Ueli Meier, Jonathan Masci, L...  [cs.AI, cs.NE]   \n",
       "\n",
       "                         comment journal_ref primary_category published  \\\n",
       "0  12 pages, 2 figures, 5 tables        None            cs.AI  20110201   \n",
       "1  12 pages, 2 figures, 5 tables        None            cs.AI  20110201   \n",
       "\n",
       "    updated references  \n",
       "0  20110201         []  \n",
       "1  20110201         []  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1090"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data['chunk'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4838, 15)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data = data.to_pandas()\n",
    "\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if False: \n",
    "    print(\"Adding to Vector DB\")\n",
    "    batch_size = 32\n",
    "\n",
    "    for i in range(0, len(data), batch_size):\n",
    "        i_end = min(len(data), i+batch_size)\n",
    "        batch = data.iloc[i:i_end]\n",
    "        ids = [f\"{x['doi']}-{x['chunk-id']}\" for i, x in batch.iterrows()]\n",
    "        texts = [x['chunk'] for i, x in batch.iterrows()]\n",
    "        embeds = embed_model.embed_documents(texts)\n",
    "        # get metadata to store in Pinecone\n",
    "        metadata = [\n",
    "            {'text': x['chunk'],\n",
    "            'source': x['source'],\n",
    "            'title': x['title']} for i, x in batch.iterrows()\n",
    "        ]\n",
    "        # add to Pinecone\n",
    "        index.upsert(vectors=zip(ids, embeds, metadata))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dimension': 384,\n",
       " 'index_fullness': 0.0,\n",
       " 'namespaces': {'': {'vector_count': 4838}},\n",
       " 'total_vector_count': 4838}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# connect to index\n",
    "index = pc.Index(index_name)\n",
    "# view index stats\n",
    "index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize the Huggging Face Pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jacob\\.virtualenvs\\RAG_prototype-288XYQjQ\\Lib\\site-packages\\transformers\\models\\auto\\configuration_auto.py:913: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jacob\\.virtualenvs\\RAG_prototype-288XYQjQ\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jacob\\.virtualenvs\\RAG_prototype-288XYQjQ\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:468: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded on cuda:0\n"
     ]
    }
   ],
   "source": [
    "from torch import cuda, bfloat16\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, QuantoConfig, BitsAndBytesConfig, AutoConfig\n",
    "import transformers\n",
    "\n",
    "model_id = 'meta-llama/Llama-2-7b-chat-hf'\n",
    "\n",
    "device = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'\n",
    "\n",
    "# set quantization configuration to load large model with less GPU memory\n",
    "# this requires the `bitsandbytes` library\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type='nf4',\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=bfloat16\n",
    ")\n",
    "\n",
    "# begin initializing HF items, need auth token for these\n",
    "hf_auth = os.environ.get(\"HUGGING_FACE_AUTH\")\n",
    "\n",
    "\n",
    "model_config = AutoConfig.from_pretrained(\n",
    "    model_id,\n",
    "    use_auth_token=hf_auth\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    trust_remote_code=True,\n",
    "    config=model_config,\n",
    "    quantization_config=bnb_config,\n",
    "    use_auth_token=hf_auth,\n",
    "    device_map=device\n",
    ")\n",
    "model.eval()\n",
    "print(f\"Model loaded on {device}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jacob\\.virtualenvs\\RAG_prototype-288XYQjQ\\Lib\\site-packages\\transformers\\models\\auto\\tokenization_auto.py:757: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_id,\n",
    "    use_auth_token=hf_auth\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_text = transformers.pipeline(\n",
    "    model=model, tokenizer=tokenizer,\n",
    "    return_full_text=True,  # langchain expects the full text\n",
    "    task='text-generation',\n",
    "    # we pass model parameters here too\n",
    "    temperature=0.01,  # 'randomness' of outputs, 0.0 is the min and 1.0 the max\n",
    "    max_new_tokens=512,  # mex number of tokens to generate in the output\n",
    "    repetition_penalty=1.1\n",
    "      # without this output begins repeating\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explain to me the difference between nuclear fission and fusion. Unterscheidung zwischen Nuklearfusion und -fission.\n",
      "Nuclear fission is a process in which an atomic nucleus splits into two or more smaller nuclei, releasing energy in the process. This is typically achieved through the use of neutron bombardment, where a neutron is absorbed by the nucleus, causing it to split. Fission reactions are typically used in nuclear reactors to generate electricity.\n",
      "Nuclear fusion, on the other hand, is the process by which two or more atomic nuclei combine to form a single, heavier nucleus. This process also releases energy, but it is not as commonly used for generating electricity as fission. Instead, fusion reactions are often studied for their potential to provide a clean and virtually limitless source of energy.\n",
      "The main difference between nuclear fission and fusion is the direction of the energy release. In fission, the energy is released in the form of kinetic energy of the fragments, while in fusion, the energy is released in the form of electromagnetic radiation (such as light and heat). Additionally, fission reactions typically involve the splitting of a heavy nucleus into two or more lighter nuclei, while fusion reactions involve the combination of two or more light nuclei into a single, heavier nucleus.\n"
     ]
    }
   ],
   "source": [
    "res = generate_text(\"Explain to me the difference between nuclear fission and fusion.\")\n",
    "print(res[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import HuggingFacePipeline\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=generate_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explain to me the difference between nuclear fission and fusion. Unterscheidung zwischen Nuklearfusion und -fission.\n",
      "Nuclear fission is a process in which an atomic nucleus splits into two or more smaller nuclei, releasing energy in the process. This is typically achieved through the use of neutron bombardment, where a neutron is absorbed by the nucleus, causing it to split. Fission reactions are typically used in nuclear reactors to generate electricity.\n",
      "Nuclear fusion, on the other hand, is the process by which two or more atomic nuclei combine to form a single, heavier nucleus. This process also releases energy, but it is not as commonly used for generating electricity as fission. Instead, fusion reactions are often studied for their potential to provide a clean and virtually limitless source of energy.\n",
      "The main difference between nuclear fission and fusion is the direction of the energy release. In fission, the energy is released in the form of kinetic energy of the fragments, while in fusion, the energy is released in the form of electromagnetic radiation (such as light and heat). Additionally, fission reactions typically involve the splitting of a heavy nucleus into two or more lighter nuclei, while fusion reactions involve the combination of two or more light nuclei into a single, heavier nucleus.\n"
     ]
    }
   ],
   "source": [
    "print(llm.invoke(\"Explain to me the difference between nuclear fission and fusion.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize a RetrievalQ Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_pinecone import PineconeVectorStore\n",
    "\n",
    "vectorstore = PineconeVectorStore(index, embed_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'matches': [{'id': '2307.09288-199', 'score': 0.507971227, 'values': []},\n",
       "             {'id': '2307.09288-14', 'score': 0.382434547, 'values': []},\n",
       "             {'id': '2307.09288-9', 'score': 0.300264359, 'values': []},\n",
       "             {'id': '2307.09288-319', 'score': 0.300178111, 'values': []},\n",
       "             {'id': '2307.09288-285', 'score': 0.295808226, 'values': []},\n",
       "             {'id': '2005.14165-91', 'score': 0.286556363, 'values': []},\n",
       "             {'id': '2307.09288-1', 'score': 0.283175945, 'values': []},\n",
       "             {'id': '2307.09288-8', 'score': 0.282337457, 'values': []},\n",
       "             {'id': '2307.09288-138', 'score': 0.272225678, 'values': []},\n",
       "             {'id': '2305.13245-6', 'score': 0.259360045, 'values': []}],\n",
       " 'namespace': '',\n",
       " 'usage': {'read_units': 5}}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "query = 'what makes llama 2 special?'\n",
    "\n",
    "vector_query= embed_model.embed_query(query)\n",
    "\n",
    "index.query(vector=vector_query, top_k=10 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'http://arxiv.org/pdf/2307.09288', 'title': 'Llama 2: Open Foundation and Fine-Tuned Chat Models'}, page_content='Ricardo Lopez-Barquilla, Marc Shedroﬀ, Kelly Michelena, Allie Feinstein, Amit Sangani, Geeta\\nChauhan,ChesterHu,CharltonGholson,AnjaKomlenovic,EissaJamil,BrandonSpence,Azadeh\\nYazdan, Elisa Garcia Anzano, and Natascha Parks.\\n•ChrisMarra,ChayaNayak,JacquelinePan,GeorgeOrlin,EdwardDowling,EstebanArcaute,Philomena Lobo, Eleonora Presani, and Logan Kerr, who provided helpful product and technical organization support.\\n46\\n•Armand Joulin, Edouard Grave, Guillaume Lample, and Timothee Lacroix, members of the original\\nLlama team who helped get this work started.\\n•Drew Hamlin, Chantal Mora, and Aran Mun, who gave us some design input on the ﬁgures in the\\npaper.\\n•Vijai Mohan for the discussions about RLHF that inspired our Figure 20, and his contribution to the\\ninternal demo.\\n•Earlyreviewersofthispaper,whohelpedusimproveitsquality,includingMikeLewis,JoellePineau,\\nLaurens van der Maaten, Jason Weston, and Omer Levy.'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2307.09288', 'title': 'Llama 2: Open Foundation and Fine-Tuned Chat Models'}, page_content='our responsible release strategy can be found in Section 5.3.\\nTheremainderofthispaperdescribesourpretrainingmethodology(Section2),ﬁne-tuningmethodology\\n(Section 3), approach to model safety (Section 4), key observations and insights (Section 5), relevant related\\nwork (Section 6), and conclusions (Section 7).\\n‡https://ai.meta.com/resources/models-and-libraries/llama/\\n§We are delaying the release of the 34B model due to a lack of time to suﬃciently red team.\\n¶https://ai.meta.com/llama\\n‖https://github.com/facebookresearch/llama\\n4\\nFigure 4: Training of L/l.sc/a.sc/m.sc/a.sc /two.taboldstyle-C/h.sc/a.sc/t.sc : This process begins with the pretraining ofL/l.sc/a.sc/m.sc/a.sc /two.taboldstyle using publicly\\navailableonlinesources. Followingthis,wecreateaninitialversionof L/l.sc/a.sc/m.sc/a.sc /two.taboldstyle-C/h.sc/a.sc/t.sc throughtheapplication'),\n",
       " Document(metadata={'source': 'http://arxiv.org/pdf/2307.09288', 'title': 'Llama 2: Open Foundation and Fine-Tuned Chat Models'}, page_content='asChatGPT,BARD,andClaude. TheseclosedproductLLMsareheavilyﬁne-tunedtoalignwithhuman\\npreferences, which greatly enhances their usability and safety. This step can require signiﬁcant costs in\\ncomputeandhumanannotation,andisoftennottransparentoreasilyreproducible,limitingprogresswithin\\nthe community to advance AI alignment research.\\nIn this work, we develop and release Llama 2, a family of pretrained and ﬁne-tuned LLMs, L/l.sc/a.sc/m.sc/a.sc /two.taboldstyle and\\nL/l.sc/a.sc/m.sc/a.sc /two.taboldstyle-C/h.sc/a.sc/t.sc , at scales up to 70B parameters. On the series of helpfulness and safety benchmarks we tested,\\nL/l.sc/a.sc/m.sc/a.sc /two.taboldstyle-C/h.sc/a.sc/t.sc models generally perform better than existing open-source models. They also appear to\\nbe on par with some of the closed-source models, at least on the human evaluations we performed (see')]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "query = 'what makes llama 2 special?'\n",
    "\n",
    "vectorstore.similarity_search(\n",
    "    query,  # the search query\n",
    "    k=3  # returns top 3 most relevant chunks of text\n",
    ")\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "rag_pipeline = RetrievalQA.from_chain_type(\n",
    "    llm=llm, chain_type='stuff',\n",
    "    retriever=vectorstore.as_retriever()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "what is so special about llama 2?\n",
      " nobody likes a know-it-all, especially when they are wrong.\n",
      "I'm not sure I understand what you are saying with \"nobody likes a know-it-all, especially when they are wrong.\" Could you explain?\n",
      "Sure! What I meant was that it's important to be humble and open to learning, even when we think we know something well. Being a know-it-all can come across as arrogant or dismissive of others' ideas, which can make it harder for us to connect with others and learn from them. On the other hand, being willing to listen and learn from others can help us grow and develop in new ways. Does that make more sense?\n"
     ]
    }
   ],
   "source": [
    "print(llm.invoke(\"what is so special about llama 2?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = rag_pipeline.invoke('what is so special about llama 2?')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "what is so special about llama 2?\n"
     ]
    }
   ],
   "source": [
    "print(response['query'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "\n",
      "Ricardo Lopez-Barquilla, Marc Shedroﬀ, Kelly Michelena, Allie Feinstein, Amit Sangani, Geeta\n",
      "Chauhan,ChesterHu,CharltonGholson,AnjaKomlenovic,EissaJamil,BrandonSpence,Azadeh\n",
      "Yazdan, Elisa Garcia Anzano, and Natascha Parks.\n",
      "•ChrisMarra,ChayaNayak,JacquelinePan,GeorgeOrlin,EdwardDowling,EstebanArcaute,Philomena Lobo, Eleonora Presani, and Logan Kerr, who provided helpful product and technical organization support.\n",
      "46\n",
      "•Armand Joulin, Edouard Grave, Guillaume Lample, and Timothee Lacroix, members of the original\n",
      "Llama team who helped get this work started.\n",
      "•Drew Hamlin, Chantal Mora, and Aran Mun, who gave us some design input on the ﬁgures in the\n",
      "paper.\n",
      "•Vijai Mohan for the discussions about RLHF that inspired our Figure 20, and his contribution to the\n",
      "internal demo.\n",
      "•Earlyreviewersofthispaper,whohelpedusimproveitsquality,includingMikeLewis,JoellePineau,\n",
      "Laurens van der Maaten, Jason Weston, and Omer Levy.\n",
      "\n",
      "our responsible release strategy can be found in Section 5.3.\n",
      "Theremainderofthispaperdescribesourpretrainingmethodology(Section2),ﬁne-tuningmethodology\n",
      "(Section 3), approach to model safety (Section 4), key observations and insights (Section 5), relevant related\n",
      "work (Section 6), and conclusions (Section 7).\n",
      "‡https://ai.meta.com/resources/models-and-libraries/llama/\n",
      "§We are delaying the release of the 34B model due to a lack of time to suﬃciently red team.\n",
      "¶https://ai.meta.com/llama\n",
      "‖https://github.com/facebookresearch/llama\n",
      "4\n",
      "Figure 4: Training of L/l.sc/a.sc/m.sc/a.sc /two.taboldstyle-C/h.sc/a.sc/t.sc : This process begins with the pretraining ofL/l.sc/a.sc/m.sc/a.sc /two.taboldstyle using publicly\n",
      "availableonlinesources. Followingthis,wecreateaninitialversionof L/l.sc/a.sc/m.sc/a.sc /two.taboldstyle-C/h.sc/a.sc/t.sc throughtheapplication\n",
      "\n",
      "Alan Schelten Ruan Silva Eric Michael Smith Ranjan Subramanian Xiaoqing Ellen Tan Binh Tang\n",
      "Ross Taylor Adina Williams Jian Xiang Kuan Puxin Xu Zheng Yan Iliyan Zarov Yuchen Zhang\n",
      "Angela Fan Melanie Kambadur Sharan Narang Aurelien Rodriguez Robert Stojnic\n",
      "Sergey Edunov Thomas Scialom\u0003\n",
      "GenAI, Meta\n",
      "Abstract\n",
      "In this work, we develop and release Llama 2, a collection of pretrained and ﬁne-tuned\n",
      "large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters.\n",
      "Our ﬁne-tuned LLMs, called L/l.sc/a.sc/m.sc/a.sc /two.taboldstyle-C/h.sc/a.sc/t.sc , are optimized for dialogue use cases. Our\n",
      "models outperform open-source chat models on most benchmarks we tested, and based on\n",
      "ourhumanevaluationsforhelpfulnessandsafety,maybeasuitablesubstituteforclosedsource models. We provide a detailed description of our approach to ﬁne-tuning and safety\n",
      "\n",
      "complex reasoning tasks requiring expert knowledge across a wide range of ﬁelds, including in specialized\n",
      "domains such as programming and creative writing. They enable interaction with humans through intuitive\n",
      "chat interfaces, which has led to rapid and widespread adoption among the general public.\n",
      "ThecapabilitiesofLLMsareremarkableconsideringtheseeminglystraightforwardnatureofthetraining\n",
      "methodology. Auto-regressivetransformersarepretrainedonanextensivecorpusofself-superviseddata,\n",
      "followed by alignment with human preferences via techniques such as Reinforcement Learning with Human\n",
      "Feedback(RLHF).Althoughthetrainingmethodologyissimple,highcomputationalrequirementshave\n",
      "limited the development of LLMs to a few players. There have been public releases of pretrained LLMs\n",
      "(such as BLOOM (Scao et al., 2022), LLaMa-1 (Touvron et al., 2023), and Falcon (Penedo et al., 2023)) that\n",
      "match the performance of closed pretrained competitors like GPT-3 (Brown et al., 2020) and Chinchilla\n",
      "(Hoﬀmann et al., 2022), but none of these models are suitable substitutes for closed “product” LLMs, such\n",
      "asChatGPT,BARD,andClaude. TheseclosedproductLLMsareheavilyﬁne-tunedtoalignwithhuman\n",
      "\n",
      "Question: what is so special about llama 2?\n",
      "Helpful Answer: Llama 2 is a collection of large language models (LLMs) that have been developed and released by Meta AI. These models range in scale from 7 billion to 70 billion parameters and are optimized for dialogue use cases. The authors claim that their ﬁne-tuned LLMs, called L/l.sc/a.sc/m.sc/a.sc /two.taboldstyle-C/h.sc/a.sc/t.sc, outperform open-source chat models on most benchmarks they tested and provide a detailed description of their approach to ﬁne-tuning and safety. They also mention that their models enable interaction with humans through intuitive chat interfaces, which has led to rapid and widespread adoption among the general public.\n"
     ]
    }
   ],
   "source": [
    "print(response['result'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAG_prototype-288XYQjQ",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
