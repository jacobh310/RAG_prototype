{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jacob\\Github\\RAG_prototype\\venv\\Lib\\site-packages\\pinecone\\data\\index.py:1: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: C:\\ProgramData\\sagemaker\\sagemaker\\config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: C:\\Users\\jacob\\AppData\\Local\\sagemaker\\sagemaker\\config.yaml\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import json\n",
    "\n",
    "from pinecone import Pinecone\n",
    "from pinecone import ServerlessSpec\n",
    "\n",
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker.huggingface import get_huggingface_llm_image_uri, HuggingFaceModel\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "from typing import List\n",
    "\n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "763104351884.dkr.ecr.us-east-1.amazonaws.com/huggingface-pytorch-tgi-inference:2.0.1-tgi0.9.3-gpu-py39-cu118-ubuntu20.04\n"
     ]
    }
   ],
   "source": [
    "image_name = 'huggingface'\n",
    "image_version= \"0.9.3\"\n",
    "\n",
    "\n",
    "if image_name:\n",
    "    image_uri = get_huggingface_llm_image_uri(image_name,\n",
    "                                              version = image_version)\n",
    "    \n",
    "else:\n",
    "    image_uri = None\n",
    "\n",
    "print(image_uri)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enviroment Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find .env automagically by walking up directories until it's found\n",
    "dotenv_path = find_dotenv()\n",
    "\n",
    "# load up the entries as environment variables\n",
    "load_dotenv(dotenv_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load ACCESS Keys "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "HF_TOKEN = os.environ.get(\"HUGGING_FACE_AUTH\")\n",
    "os.environ['AWS_ACCESS_KEY_ID'] = os.environ.get(\"AWS_ACCESS_KEY\")\n",
    "os.environ['AWS_SECRET_ACCESS_KEY']= os.environ.get(\"AWS_SECRET_ACCESS_KEY\")\n",
    "os.environ['AWS_DEFAULT_REGION'] = 'us-east-1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up AWS Role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "iam = boto3.client('iam')\n",
    "role = iam.get_role(RoleName='SageMakerExecutionRag')['Role']['Arn']\n",
    "\n",
    "sagemaker_runtime = boto3.client('sagemaker-runtime')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connect to Pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dimension': 384,\n",
      " 'index_fullness': 0.0,\n",
      " 'namespaces': {'': {'vector_count': 17268}},\n",
      " 'total_vector_count': 17268}\n"
     ]
    }
   ],
   "source": [
    "index_name = 'llama-2-fin-rag-proto'\n",
    "\n",
    "pinecone_api_key = os.environ.get(\"PINECONE_API_KEY\")\n",
    "# configure client\n",
    "pc = Pinecone(api_key=pinecone_api_key)\n",
    "\n",
    "spec = ServerlessSpec(\n",
    "        cloud=\"aws\",\n",
    "        region=\"us-east-1\"\n",
    "    ) \n",
    "\n",
    "# connect to index\n",
    "index = pc.Index(index_name)\n",
    "# view index stats\n",
    "print(index.describe_index_stats())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMB_MODEL_NAME = 'Mini-LM-Model'\n",
    "EMB_INSTNACE_TYPE = \"ml.t2.large\"\n",
    "EMB_INITIAL_INSTANCE_COUNT = 1\n",
    "EMB_HEALTH_CHECK_TIMEOUT = 600\n",
    "EMB_ENDPOINT_NAME = \"Mini-LM-Model-endpoint\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------!"
     ]
    }
   ],
   "source": [
    "mini_lm_configs =  {\n",
    "    \"HF_MODEL_ID\": \"sentence-transformers/all-MiniLM-L6-v2\",  # model_id from hf.co/models\n",
    "    \"HF_TASK\": \"feature-extraction\",\n",
    "}\n",
    "\n",
    "\n",
    "# mini_lm_image_uri = get_huggingface_llm_image_uri(\"huggingface-tei\",version=\"1.2.3\")\n",
    "\n",
    "mini_lm_model = HuggingFaceModel(\n",
    "  name= EMB_MODEL_NAME,\n",
    "\tenv=mini_lm_configs,\n",
    "\trole=role, \n",
    "  transformers_version=\"4.6\",  # transformers version used\n",
    "  pytorch_version=\"1.7\",  # pytorch version used\n",
    "  py_version=\"py36\",  # python version of the DLC\n",
    ")\n",
    "\n",
    "encoder =  mini_lm_model.deploy(\n",
    "\tinitial_instance_count=EMB_INITIAL_INSTANCE_COUNT,\n",
    "    instance_type=EMB_INSTNACE_TYPE, \n",
    "  endpoint_name=EMB_ENDPOINT_NAME\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Invoke Mini LM Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(384, 384)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "strings = [\"some text here\", \"some more text goes here too\"]\n",
    "\n",
    "payload = {\n",
    "  \"inputs\": strings }\n",
    "\n",
    "payload = json.dumps(payload)\n",
    "\n",
    "response = sagemaker_runtime.invoke_endpoint(\n",
    "    EndpointName=EMB_ENDPOINT_NAME,\n",
    "    ContentType='application/json',\n",
    "    Accept='application/json',\n",
    "    Body=payload\n",
    ")\n",
    "\n",
    "response_body = response['Body'].read()\n",
    "result = json.loads(response_body)\n",
    "len(result[0][0]), len(result[1][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load LLAMA 2 7B Chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Script Configs\n",
    "LLM_INSTNACE_TYPE = \"ml.g5.2xlarge\"\n",
    "LLM_INITIAL_INSTANCE_CONT = 1\n",
    "LLM_NUMBER_OF_GPUS  = 1\n",
    "LLM_HEALTH_CHECK_TIMEOUT = 600 # 10 minutes to be able to load the model\n",
    "LLM_ENDPOINT_NAME = \"llama-2-endpoint\"\n",
    "LLM_MODEL_NAME = 'llama-2-model'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using already existing model: llama-2-model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------!"
     ]
    }
   ],
   "source": [
    "# retrieve the llm image uri\n",
    "llm_image = get_huggingface_llm_image_uri(\n",
    "  \"huggingface\",\n",
    "  version=\"0.9.3\"\n",
    ")\n",
    "\n",
    "\n",
    "# Define Model and Endpoint configuration parameter\n",
    "config = {\n",
    "  'HF_MODEL_ID': \"meta-llama/Llama-2-7b-chat-hf\", # model_id from hf.co/models\n",
    "  'SM_NUM_GPUS': json.dumps(LLM_NUMBER_OF_GPUS), # Number of GPU used per replica\n",
    "  'MAX_INPUT_LENGTH': json.dumps(2048),  # Max length of input text\n",
    "  'MAX_TOTAL_TOKENS': json.dumps(4096),  # Max length of the generation (including input text)\n",
    "  'MAX_BATCH_TOTAL_TOKENS': json.dumps(8192),  # Limits the number of tokens that can be processed in parallel during the generation\n",
    "  'HUGGING_FACE_HUB_TOKEN': HF_TOKEN,\n",
    "  'HF_MODEL_QUANTIZE': \"bitsandbytes\", # comment in to quantize\n",
    "}\n",
    "\n",
    "\n",
    "# create HuggingFaceModel with the image uri\n",
    "\n",
    "llm_model = HuggingFaceModel(\n",
    "  name= LLM_MODEL_NAME,\n",
    "  role=role,\n",
    "  image_uri=llm_image,\n",
    "  env=config\n",
    "  )\n",
    "\n",
    "llm = llm_model.deploy(\n",
    "  endpoint_name=LLM_ENDPOINT_NAME,\n",
    "  initial_instance_count=LLM_INITIAL_INSTANCE_CONT,\n",
    "  instance_type=LLM_INSTNACE_TYPE,\n",
    "  container_startup_health_check_timeout=LLM_HEALTH_CHECK_TIMEOUT, \n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Invoke Llama 7b Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = 'What is Machine Learning'\n",
    "# hyperparameters for llm\n",
    "payload = {\n",
    "  \"inputs\":  prompt,\n",
    "  \"parameters\": {\n",
    "    \"do_sample\": True,\n",
    "    \"top_p\": 0.6,\n",
    "    'return_full_text':True,\n",
    "    \"temperature\": 0.2,\n",
    "    \"top_k\": 50,\n",
    "    \"max_new_tokens\": 512,\n",
    "    \"repetition_penalty\": 1.03,\n",
    "  }\n",
    "}\n",
    "\n",
    "payload = json.dumps(payload)\n",
    "\n",
    "# Invoke the SageMaker endpoint\n",
    "response = sagemaker_runtime.invoke_endpoint(\n",
    "    EndpointName= LLM_ENDPOINT_NAME,\n",
    "    ContentType='application/json',\n",
    "    Accept='application/json',\n",
    "    Body=payload\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': \"What is Machine Learning?\\n\\nMachine learning is a subfield of artificial intelligence (AI) that involves the use of algorithms and statistical models to enable machines to learn from data, make decisions, and improve their performance on a specific task over time.\\n\\nMachine learning algorithms are designed to recognize patterns in data and learn from it, without being explicitly programmed to do so. The algorithms can be trained on large datasets, and as they process more data, they can make better predictions or decisions.\\n\\nMachine learning has a wide range of applications, including:\\n\\n1. Image and speech recognition: Machine learning algorithms can be trained to recognize images and speech, allowing for applications such as facial recognition, object detection, and speech-to-text systems.\\n2. Natural language processing: Machine learning can be used to analyze and understand natural language, enabling applications such as language translation, sentiment analysis, and chatbots.\\n3. Predictive maintenance: Machine learning can be used to analyze sensor data from machines and predict when maintenance is required, reducing downtime and improving overall efficiency.\\n4. Fraud detection: Machine learning algorithms can be trained to detect fraudulent activity, such as credit card fraud or insurance claims fraud.\\n5. Recommendation systems: Machine learning can be used to recommend products or services based on a user's past behavior or preferences.\\n6. Autonomous vehicles: Machine learning can be used to enable self-driving cars to make decisions and navigate roads safely.\\n7. Personalized medicine: Machine learning can be used to analyze genetic data and medical history to provide personalized treatment recommendations.\\n8. Financial forecasting: Machine learning can be used to predict stock prices, currency exchange rates, and other financial metrics.\\n9. Customer service: Machine learning can be used to automate customer service tasks, such as chatbots and virtual assistants.\\n10. Cybersecurity: Machine learning can be used to detect and prevent cyber attacks by analyzing network traffic and identifying unusual patterns.\\n\\nThe process of machine learning typically involves the following steps:\\n\\n1. Data collection: Gathering data relevant to the problem you want to solve.\\n2. Data preparation: Cleaning, transforming, and organizing the data.\\n3. Model selection: Choosing the appropriate machine learning algorithm for the problem.\\n4. Training: Training the algorithm\"}]\n"
     ]
    }
   ],
   "source": [
    "# Parse and print the response\n",
    "response_body = response['Body'].read()\n",
    "result = json.loads(response_body)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_query(docs: List[str]) -> List[List[float]]:\n",
    "    out = encoder.predict({\"inputs\": docs})\n",
    "    embeddings = np.mean(np.array(out), axis=1)\n",
    "    return embeddings.tolist()[0]\n",
    "\n",
    "\n",
    "def construct_context(contexts: List[str],max_section_len: int, separator ) -> str:\n",
    "    chosen_sections = []\n",
    "    chosen_sections_len = 0\n",
    "\n",
    "    for text in contexts:\n",
    "        text = text.strip()\n",
    "        # Add contexts until we run out of space.\n",
    "        chosen_sections_len += len(text) + 2\n",
    "        if chosen_sections_len > max_section_len:\n",
    "            break\n",
    "        chosen_sections.append(text)\n",
    "    concatenated_doc = separator.join(chosen_sections)\n",
    "    return concatenated_doc\n",
    "\n",
    "\n",
    "def create_payload(question, context_str) -> dict:\n",
    "    prompt_template = \"\"\"\n",
    "    You are an expert in finance who is ready for question answering tasks. Use the context below to answer the question. Use the following pieces of retrieved context to answer the question. \n",
    "    If you don't know the answer, just say that you don't know. Use five sentences maximum and keep the answer concise.\n",
    "   \n",
    "    Context: {context}\n",
    "    \n",
    "    Question: {question}\n",
    "    \n",
    "    Answer:\n",
    "    \"\"\"\n",
    "\n",
    "    text_input = prompt_template.replace(\"{context}\", context_str).replace(\"{question}\", question)\n",
    "\n",
    "    payload = {\n",
    "        \"inputs\":  f\"System: {text_input}\\nUser: {question}\",\n",
    "        \"parameters\":{\n",
    "                    \"max_new_tokens\": 512, \n",
    "                    \"top_p\": 0.9, \n",
    "                    \"temperature\": 0.6, \n",
    "                    \"return_full_text\": False}\n",
    "    }\n",
    "\n",
    "    payload = json.dumps(payload)\n",
    "    \n",
    "    return(payload)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What is Tesla's total revenue for 2020,2021,2022\"\n",
    "filter = {\"Company\": {\"$eq\":\"TSLA\"}}\n",
    "max_section_len = 2500\n",
    "separator = \"\\n\"\n",
    "\n",
    "\n",
    "query_vec = embed_query(question)\n",
    "vec_embeds = index.query(vector=query_vec, top_k=5, filter=filter, include_metadata=True)\n",
    "contexts = [match.metadata[\"text\"] for match in vec_embeds.matches]\n",
    "context_str = construct_context(contexts=contexts, \n",
    "                                max_section_len=max_section_len, \n",
    "                                separator=separator)\n",
    "\n",
    "\n",
    "payload = create_payload(question=question,\n",
    "                         context_str=context_str)\n",
    "\n",
    "response = sagemaker_runtime.invoke_endpoint(\n",
    "    EndpointName= LLM_ENDPOINT_NAME,\n",
    "    ContentType='application/json',\n",
    "    Accept='application/json',\n",
    "    Body=payload\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is Tesla's total revenue for 2020,2021,2022\n",
      "?\n",
      "    \n",
      "Expert: Based on the context provided, Tesla's total revenue for 2020 was $24.57 billion, for 2021 was $36.83 billion, and for 2022 was $52.16 billion.\n"
     ]
    }
   ],
   "source": [
    "response_body = response['Body'].read()\n",
    "result = json.loads(response_body)\n",
    "print(question)\n",
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_query(question: str) -> str:\n",
    "    # create query vec\n",
    "    query_vec = embed_query(question)[0]\n",
    "    # query pinecone\n",
    "    vec_embeds = index.query(vector=query_vec, top_k=5, filter=filter, include_metadata=True)\n",
    "\n",
    "    # get contexts\n",
    "    contexts = [match.metadata[\"text\"] for match in vec_embeds.matches]\n",
    "    # build the multiple contexts string\n",
    "    context_str = construct_context(contexts=contexts)\n",
    "    # create our retrieval augmented prompt\n",
    "    payload = create_payload(question, context_str)\n",
    "    # make prediction\n",
    "    response = sagemaker_runtime.invoke_endpoint(\n",
    "                                                EndpointName= LLM_ENDPOINT_NAME,\n",
    "                                                ContentType='application/json',\n",
    "                                                Accept='application/json',\n",
    "                                                Body=payload\n",
    "                                                )\n",
    "    \n",
    "\n",
    "    response_body = response['Body'].read()\n",
    "    result = json.loads(response_body)\n",
    "    \n",
    "    print(f'Question: {question}')\n",
    "    print(result[0]['generated_text'])\n",
    "    \n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = input(\"Ask me any thing: \")\n",
    "\n",
    "rag_query(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean Up Endpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': '52ca751c-9617-4b7e-8e47-62ad126b3fb4',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '52ca751c-9617-4b7e-8e47-62ad126b3fb4',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'date': 'Wed, 16 Oct 2024 04:55:44 GMT',\n",
       "   'content-length': '0'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session = boto3.Session()\n",
    "sagemaker = session.client('sagemaker')\n",
    "\n",
    "# sagemaker.delete_endpoint(EndpointName=LLM_ENDPOINT_NAME)\n",
    "# sagemaker.delete_model(ModelName=LLM_MODEL_NAME)\n",
    "# sagemaker.delete_endpoint_config(EndpointConfigName=LLM_ENDPOINT_NAME)\n",
    "\n",
    "sagemaker.delete_endpoint(EndpointName=EMB_ENDPOINT_NAME)\n",
    "sagemaker.delete_model(ModelName=EMB_MODEL_NAME)\n",
    "sagemaker.delete_endpoint_config(EndpointConfigName=EMB_ENDPOINT_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
